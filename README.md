# GPT Model from Scratch

This repository contains an implementation of a GPT model from scratch, covering everything from tokenization to text generation.

## Features
- Custom Tokenizer
- Byte Pair Encoding (BPE)
- Data Loader for Sequence Preparation
- Positional and Token Embeddings
- Self-Attention and Multi-Head Attention
- Transformer Blocks with Feedforward Layers
- Causal Attention for Autoregressive Processing
- Full GPT Model Implementation
- Text Generation

## Installation
```bash
pip install -r requirements.txt
```

## Usage
Run the training script:
```bash
python train.py
```

Generate text:
```bash
python generate.py
```
